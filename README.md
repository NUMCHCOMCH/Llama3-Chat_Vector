#  Llama3-Chat_Vector-kor_Instruct : Chat-Vectorë¥¼ í™œìš©í•œ í•œêµ­ì–´ LLAMA3 ëª¨ë¸
- Chat-Vector Paper(https://arxiv.org/abs/2310.04799)
<p align="center" width="100%">
<img src="assert/ocelot.png" alt="NLP Logo" style="width: 50%;">
</p>

## Update Logs
- 2024.06.30: [ğŸ¤—Llama3 ëª¨ë¸ ê³µê°œ](cpm-ai/Ocelot-Ko-self-instruction-10.8B-v1.0)
ì´ ëª¨ë¸ì€ Llama3-8B ëª¨ë¸ì˜ Instruct ë²„ì „ì— í•´ë‹¹í•©ë‹ˆë‹¤.
---

### Reference Models:
1) meta-llama/Meta-Llama-3-8B(https://huggingface.co/meta-llama/Meta-Llama-3-8B)
2) meta-llama/Meta-Llama-3-8B-Instruct(https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
3) beomi/Llama-3-KoEn-8B(https://huggingface.co/beomi/Llama-3-KoEn-8B)

**Model Developers**: nebchi

## Model Information
* Chat_VectorëŠ” í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ë”í•˜ê³  ë¹¼ëŠ” ê²ƒìœ¼ë¡œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì— ëŒ€í™”ëŠ¥ë ¥ì„ ë¶€ì—¬ë¥¼ í•˜ëŠ”ë°, ì´ë¥¼ í†µí•´ ì˜ì–´ ì¤‘ì‹¬ìœ¼ë¡œ í•™ìŠµëœ LLAVA ëª¨ë¸ì— ëŒ€ëŸ‰ì˜ í•œêµ­ì–´ ì½”í¼ìŠ¤ë¡œ í•™ìŠµí•œ í•œêµ­ì–´ LLMì˜ ì–¸ì–´ ëŠ¥ë ¥ì„ LLAVAì— ì£¼ì…í•˜ì—¬ í•œêµ­ì–´ ë‹µë³€ì´ ê°€ëŠ¥í•˜ë„ë¡ í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤.

### Description
* ì´ë²ˆ Llama3-Chat_Vector-kor_Instruct ëª¨ë¸ì€ ëŒ€ëŸ‰ì˜ í•œêµ­ì–´ ì½”í¼ìŠ¤ë¡œ ì‚¬ì „í•™ìŠµí•œ LLAMA ëª¨ë¸ì—ì„œ, ì±„íŒ… ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë”í•´ ë¯¸ì„¸ì¡°ì • ì—†ì´ Instruction Modelë¡œ ê°œë°œí•œ sLLMì…ë‹ˆë‹¤.

### Inputs and outputs
*   **Input:** ì§ˆë¬¸, í”„ë¡¬í”„íŠ¸ ë˜ëŠ” êµì •ì„ ìœ„í•œ ë¬¸ì„œì™€ ê°™ì€ í…ìŠ¤íŠ¸ ë¬¸ìì—´.
*   **Output:** ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µìœ¼ë¡œ ìƒì„±ëœ í•œêµ­ì–´ í…ìŠ¤íŠ¸. ì˜ˆë¥¼ ë“¤ì–´, ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ë‚˜ ì´ë ¥ì„œ í‰ê°€.

---

#### ëª¨ë¸ ì‹¤í–‰ ì˜ˆì‹œ ì½”ë“œ / multi gpu
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TextStreamer
import torch

tokenizer = AutoTokenizer.from_pretrained(
    "nebchi/Llama3-Chat_Vector-kor",
)

model = AutoModelForCausalLM.from_pretrained(
    "nebchi/Llama3-Chat_Vector-kor",
    torch_dtype=torch.bfloat16,
    device_map='auto',
)
streamer = TextStreamer(tokenizer)

messages = [
    {"role": "system", "content": "ë‹¹ì‹ ì€ ì¸ê³µì§€ëŠ¥ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë¬»ëŠ” ë§ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."},
    {"role": "user", "content": "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì— ëŒ€í•´ ì•Œë ¤ì¤˜"},
]

input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

outputs = model.generate(
    input_ids,
    max_new_tokens=512,
    eos_token_id=terminators,
    do_sample=False,
    repetition_penalty=1.05,
    streamer = streamer
)
response = outputs[0][input_ids.shape[-1]:]
print(tok

### results
```python
ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì„œìš¸íŠ¹ë³„ì‹œì…ë‹ˆë‹¤.
ì„œìš¸íŠ¹ë³„ì‹œì—ëŠ” ì²­ì™€ëŒ€, êµ­íšŒì˜ì‚¬ë‹¹, ëŒ€ë²•ì› ë“± ëŒ€í•œë¯¼êµ­ì˜ ì£¼ìš” ì •ë¶€ê¸°ê´€ì´ ìœ„ì¹˜í•´ ìˆìŠµë‹ˆë‹¤.
ë˜í•œ ì„œìš¸ì‹œëŠ” ëŒ€í•œë¯¼êµ­ì˜ ê²½ì œ, ë¬¸í™”, êµìœ¡, êµí†µì˜ ì¤‘ì‹¬ì§€ë¡œì¨ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ì ëŒ€í‘œ ë„ì‹œì…ë‹ˆë‹¤.ì œê°€ ë„ì›€ì´ ë˜ì—ˆê¸¸ ë°”ëë‹ˆë‹¤. ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”!
```
---

**Citation**

```bibtex
@misc {Llama3-Chat_Vector-kor_Instruct,
	author       = { {nebchi} },
	title        = { Llama3-Chat_Vector-kor_Instruct },
	year         = 2024,
	url          = { https://huggingface.co/nebchi/Llama3-Chat_Vector-kor_llava },
	publisher    = { Hugging Face }
}
``'
}
```
